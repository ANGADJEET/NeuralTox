{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "309bbee5-0e1d-49c2-b143-18566db404ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-09 20:44:46.878510: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-09-09 20:44:46.896001: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-09-09 20:44:46.915524: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-09-09 20:44:46.921284: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-09-09 20:44:46.936129: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-09-09 20:44:47.879773: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Import Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report, roc_auc_score, precision_score, recall_score, roc_curve, precision_recall_curve, matthews_corrcoef\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LeakyReLU, Dropout, LayerNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "print(\"Libraries loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "756dbbc0-a830-4895-98b8-d3df63801f42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data preprocessing completed.\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Load and preprocess dataset\n",
    "df = pd.read_csv('merged_data_clean.csv')\n",
    "df = df[(df['Source'] == 'ToxiM') | (df['Source'] == 'MolToxPred')]\n",
    "categorical_columns = ['SMILES', 'Source']\n",
    "df = df.drop(columns=categorical_columns)\n",
    "Y = df['Toxicity']\n",
    "# X_pca_df = pd.read_csv('X_boruta_df_ToxiM_and_MoltoxPred.csv')\n",
    "X_pca_df = pd.read_csv('X_pca_clean_with_boruta_ToxiM_and_MolToxPred.csv')\n",
    "# Handle class imbalance using SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "X_resampled, Y_resampled = smote.fit_resample(X_pca_df, Y)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_resampled, Y_resampled, test_size=0.2, random_state=42, shuffle=True, stratify=Y_resampled)\n",
    "\n",
    "print(\"Data preprocessing completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9fbbfb7e-8c4f-4257-a0b3-8d902ba24164",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reloading Tuner from tuner_dir/hyperparameter_tuning_bayes/tuner0.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_930/764455109.py:2: DeprecationWarning: `import kerastuner` is deprecated, please use `import keras_tuner`.\n",
      "  from kerastuner.tuners import BayesianOptimization\n",
      "2024-09-09 20:46:16.941676: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2343] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "/home/angadjeet22071/miniconda3/lib/python3.11/site-packages/keras/src/layers/activations/leaky_relu.py:41: UserWarning: Argument `alpha` is deprecated. Use `negative_slope` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - accuracy: 0.6566 - loss: 5.5983 - val_accuracy: 0.7852 - val_loss: 4.3343 - learning_rate: 1.6803e-04\n",
      "Epoch 2/300\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.7686 - loss: 4.0928 - val_accuracy: 0.8071 - val_loss: 3.2992 - learning_rate: 1.6803e-04\n",
      "Epoch 3/300\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.8164 - loss: 3.0908 - val_accuracy: 0.8278 - val_loss: 2.6032 - learning_rate: 1.6803e-04\n",
      "Epoch 4/300\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.8260 - loss: 2.4492 - val_accuracy: 0.8361 - val_loss: 2.1254 - learning_rate: 1.6803e-04\n",
      "Epoch 5/300\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.8399 - loss: 1.9746 - val_accuracy: 0.8328 - val_loss: 1.7840 - learning_rate: 1.6803e-04\n",
      "Epoch 6/300\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.8624 - loss: 1.6266 - val_accuracy: 0.8336 - val_loss: 1.5285 - learning_rate: 1.6803e-04\n",
      "Epoch 7/300\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.8589 - loss: 1.3815 - val_accuracy: 0.8431 - val_loss: 1.3545 - learning_rate: 1.6803e-04\n",
      "Epoch 8/300\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.8657 - loss: 1.1957 - val_accuracy: 0.8419 - val_loss: 1.2038 - learning_rate: 1.6803e-04\n",
      "Epoch 9/300\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.8893 - loss: 1.0256 - val_accuracy: 0.8415 - val_loss: 1.1099 - learning_rate: 1.6803e-04\n",
      "Epoch 10/300\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.8848 - loss: 0.9127 - val_accuracy: 0.8411 - val_loss: 1.0264 - learning_rate: 1.6803e-04\n",
      "Epoch 11/300\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.8948 - loss: 0.8126 - val_accuracy: 0.8382 - val_loss: 0.9453 - learning_rate: 1.6803e-04\n",
      "Epoch 12/300\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.8894 - loss: 0.7654 - val_accuracy: 0.8394 - val_loss: 0.9071 - learning_rate: 1.6803e-04\n",
      "Epoch 13/300\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9005 - loss: 0.6901 - val_accuracy: 0.8369 - val_loss: 0.8692 - learning_rate: 1.6803e-04\n",
      "Epoch 14/300\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9068 - loss: 0.6288 - val_accuracy: 0.8344 - val_loss: 0.8476 - learning_rate: 1.6803e-04\n",
      "Epoch 15/300\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9060 - loss: 0.5994 - val_accuracy: 0.8419 - val_loss: 0.7910 - learning_rate: 1.6803e-04\n",
      "Epoch 16/300\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9083 - loss: 0.5543 - val_accuracy: 0.8365 - val_loss: 0.7815 - learning_rate: 1.6803e-04\n",
      "Epoch 17/300\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9162 - loss: 0.5226 - val_accuracy: 0.8394 - val_loss: 0.7938 - learning_rate: 1.6803e-04\n",
      "Epoch 18/300\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9135 - loss: 0.4909 - val_accuracy: 0.8365 - val_loss: 0.7427 - learning_rate: 1.6803e-04\n",
      "Epoch 19/300\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9154 - loss: 0.4738 - val_accuracy: 0.8340 - val_loss: 0.7628 - learning_rate: 1.6803e-04\n",
      "Epoch 20/300\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9211 - loss: 0.4489 - val_accuracy: 0.8361 - val_loss: 0.7278 - learning_rate: 1.6803e-04\n",
      "Epoch 21/300\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9241 - loss: 0.4259 - val_accuracy: 0.8369 - val_loss: 0.7229 - learning_rate: 1.6803e-04\n",
      "Epoch 22/300\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9249 - loss: 0.4068 - val_accuracy: 0.8340 - val_loss: 0.7219 - learning_rate: 1.6803e-04\n",
      "Epoch 23/300\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9278 - loss: 0.3892 - val_accuracy: 0.8357 - val_loss: 0.7007 - learning_rate: 1.6803e-04\n",
      "Epoch 24/300\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9249 - loss: 0.3852 - val_accuracy: 0.8320 - val_loss: 0.7052 - learning_rate: 1.6803e-04\n",
      "Epoch 25/300\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9293 - loss: 0.3696 - val_accuracy: 0.8336 - val_loss: 0.7093 - learning_rate: 1.6803e-04\n",
      "Epoch 26/300\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9346 - loss: 0.3514 - val_accuracy: 0.8344 - val_loss: 0.6700 - learning_rate: 1.6803e-04\n",
      "Epoch 27/300\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9346 - loss: 0.3420 - val_accuracy: 0.8291 - val_loss: 0.7002 - learning_rate: 1.6803e-04\n",
      "Epoch 28/300\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9295 - loss: 0.3528 - val_accuracy: 0.8349 - val_loss: 0.6996 - learning_rate: 1.6803e-04\n",
      "Epoch 29/300\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9313 - loss: 0.3400 - val_accuracy: 0.8299 - val_loss: 0.7195 - learning_rate: 1.6803e-04\n",
      "Epoch 30/300\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9379 - loss: 0.3194 - val_accuracy: 0.8365 - val_loss: 0.6896 - learning_rate: 1.6803e-04\n",
      "Epoch 31/300\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9326 - loss: 0.3224 - val_accuracy: 0.8377 - val_loss: 0.6954 - learning_rate: 1.6803e-04\n",
      "Epoch 32/300\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9378 - loss: 0.3149 - val_accuracy: 0.8382 - val_loss: 0.6863 - learning_rate: 1.6803e-04\n",
      "Epoch 33/300\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9379 - loss: 0.3086 - val_accuracy: 0.8386 - val_loss: 0.7002 - learning_rate: 1.6803e-04\n",
      "Epoch 34/300\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9456 - loss: 0.2979 - val_accuracy: 0.8266 - val_loss: 0.6807 - learning_rate: 1.6803e-04\n",
      "Epoch 35/300\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9433 - loss: 0.2933 - val_accuracy: 0.8353 - val_loss: 0.7164 - learning_rate: 1.6803e-04\n",
      "Epoch 36/300\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9479 - loss: 0.2844 - val_accuracy: 0.8315 - val_loss: 0.6950 - learning_rate: 1.6803e-04\n",
      "Epoch 37/300\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9474 - loss: 0.2785 - val_accuracy: 0.8320 - val_loss: 0.7190 - learning_rate: 1.0000e-04\n",
      "Epoch 38/300\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9534 - loss: 0.2524 - val_accuracy: 0.8324 - val_loss: 0.7177 - learning_rate: 1.0000e-04\n",
      "Epoch 39/300\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9544 - loss: 0.2529 - val_accuracy: 0.8336 - val_loss: 0.7215 - learning_rate: 1.0000e-04\n",
      "Epoch 40/300\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9501 - loss: 0.2522 - val_accuracy: 0.8353 - val_loss: 0.7278 - learning_rate: 1.0000e-04\n",
      "Epoch 41/300\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9590 - loss: 0.2338 - val_accuracy: 0.8344 - val_loss: 0.7383 - learning_rate: 1.0000e-04\n",
      "Epoch 42/300\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9539 - loss: 0.2326 - val_accuracy: 0.8324 - val_loss: 0.7456 - learning_rate: 1.0000e-04\n",
      "Epoch 43/300\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9558 - loss: 0.2308 - val_accuracy: 0.8286 - val_loss: 0.7399 - learning_rate: 1.0000e-04\n",
      "Epoch 44/300\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9515 - loss: 0.2304 - val_accuracy: 0.8357 - val_loss: 0.7392 - learning_rate: 1.0000e-04\n",
      "Epoch 45/300\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9534 - loss: 0.2257 - val_accuracy: 0.8344 - val_loss: 0.7642 - learning_rate: 1.0000e-04\n",
      "Epoch 46/300\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9553 - loss: 0.2231 - val_accuracy: 0.8307 - val_loss: 0.7405 - learning_rate: 1.0000e-04\n",
      "Deep Neural Network trained.\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Define and train Deep Neural Network with Keras Tuner\n",
    "from kerastuner.tuners import BayesianOptimization\n",
    "\n",
    "def build_model(hp):\n",
    "    model = Sequential()\n",
    "    for i in range(hp.Int('num_layers', 2, 6)):\n",
    "        model.add(Dense(units=hp.Int(f'units_{i}', min_value=32, max_value=512, step=32), \n",
    "                        kernel_regularizer=tf.keras.regularizers.l2(0.01)))\n",
    "        model.add(LeakyReLU(alpha=0.01))\n",
    "        model.add(LayerNormalization())\n",
    "        model.add(Dropout(hp.Float(f'dropout_{i}', min_value=0.3, max_value=0.5, step=0.05)))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(optimizer=Adam(learning_rate=hp.Float('learning_rate', 1e-4, 1e-2, sampling='LOG')), \n",
    "                  loss='binary_crossentropy', \n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "tuner = BayesianOptimization(build_model, \n",
    "                             objective='val_accuracy', \n",
    "                             max_trials=20, \n",
    "                             executions_per_trial=2, \n",
    "                             directory='tuner_dir', \n",
    "                             project_name='hyperparameter_tuning_bayes')\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=10, min_lr=0.0001)\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
    "\n",
    "tuner.search(X_train, Y_train, epochs=50, validation_split=0.2, callbacks=[reduce_lr, early_stopping])\n",
    "\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "dnn_model = tuner.hypermodel.build(best_hps)\n",
    "history = dnn_model.fit(X_train, Y_train, epochs=300, batch_size=32, validation_split=0.2, verbose=1, callbacks=[reduce_lr, early_stopping])\n",
    "\n",
    "print(\"Deep Neural Network trained.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "255d1545-9dfd-4355-8df7-f186a75fe1d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model is running LR\n",
      "This model is running DT\n",
      "This model is running RF\n",
      "This model is running GB\n",
      "This model is running XGB\n",
      "This model is running SVM\n",
      "This model is running KNN\n",
      "This model is running NB\n",
      "Traditional machine learning models trained.\n"
     ]
    }
   ],
   "source": [
    "models = {\n",
    "    'LR': LogisticRegression(random_state=42,max_iter=200, C =0.01),\n",
    "    'DT': DecisionTreeClassifier(random_state = 42,max_depth=10),\n",
    "    'RF': RandomForestClassifier(random_state = 42, n_estimators=100),\n",
    "    'GB': GradientBoostingClassifier(random_state=42,learning_rate=0.2),\n",
    "    'XGB': XGBClassifier(random_state = 42,use_label_encoder=False, eval_metric='logloss', learning_rate=0.1),\n",
    "    'SVM': SVC(probability=True, C=1),\n",
    "    'KNN': KNeighborsClassifier(n_neighbors=7),\n",
    "    'NB': GaussianNB()\n",
    "}\n",
    "\n",
    "fitted_models = {}\n",
    "for model_name, model in models.items():\n",
    "    print(\"This model is running\", model_name)\n",
    "    model.fit(X_train, Y_train)  # Fit the model to the training data\n",
    "    fitted_models[model_name] = model  # Store the trained model\n",
    "fitted_models['DNN'] = dnn_model;\n",
    "print(\"Traditional machine learning models trained.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2ffd36eb-0e9a-45bb-ad23-6eeadbf1a262",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating DNN...\n",
      "\u001b[1m378/378\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m378/378\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "Evaluating LR...\n",
      "Evaluating DT...\n",
      "Evaluating RF...\n",
      "Evaluating GB...\n",
      "Evaluating XGB...\n",
      "Evaluating SVM...\n",
      "Evaluating KNN...\n",
      "Evaluating NB...\n",
      "Model metrics saved to 'model_metrics.csv'.\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Evaluate all models and store metrics\n",
    "def evaluate_models(models, X_train, X_test, Y_train, Y_test):\n",
    "    results = []\n",
    "    for model_name, model in models.items():\n",
    "        print(f\"Evaluating {model_name}...\")\n",
    "        if model_name == 'DNN':\n",
    "            train_pred = (model.predict(X_train) > 0.5).astype(int)\n",
    "            test_pred = (model.predict(X_test) > 0.5).astype(int)\n",
    "            train_pred_prob = model.predict(X_train).ravel()\n",
    "            test_pred_prob = model.predict(X_test).ravel()\n",
    "        else:\n",
    "            train_pred = model.predict(X_train)\n",
    "            test_pred = model.predict(X_test)\n",
    "            train_pred_prob = model.predict_proba(X_train)[:, 1]\n",
    "            test_pred_prob = model.predict_proba(X_test)[:, 1]\n",
    "        \n",
    "        metrics = {\n",
    "            'Model': model_name,\n",
    "            'Train Accuracy': accuracy_score(Y_train, train_pred),\n",
    "            'Test Accuracy': accuracy_score(Y_test, test_pred),\n",
    "            'Train F1': f1_score(Y_train, train_pred),\n",
    "            'Test F1': f1_score(Y_test, test_pred),\n",
    "            'Train MCC': matthews_corrcoef(Y_train, train_pred),\n",
    "            'Test MCC': matthews_corrcoef(Y_test, test_pred),\n",
    "            'Train ROC AUC': roc_auc_score(Y_train, train_pred_prob),\n",
    "            'Test ROC AUC': roc_auc_score(Y_test, test_pred_prob),\n",
    "            'Train Precision': precision_score(Y_train, train_pred),\n",
    "            'Test Precision': precision_score(Y_test, test_pred),\n",
    "            'Train Recall': recall_score(Y_train, train_pred),\n",
    "            'Test Recall': recall_score(Y_test, test_pred)\n",
    "        }\n",
    "        results.append(metrics)\n",
    "    \n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df.to_csv(\"model_metrics_latest.csv\")\n",
    "    print(\"Model metrics saved to 'model_metrics.csv'.\")\n",
    "\n",
    "evaluate_models({'DNN': dnn_model, **fitted_models}, X_train, X_test, Y_train, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "da0499e5-ebf3-45e7-8ff8-7fbe9b8f1f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, precision_recall_curve\n",
    "\n",
    "def plot_curves(models, X_train, X_test, Y_train, Y_test):\n",
    "    plt.rcParams.update({'font.size': 12})\n",
    "    \n",
    "    # ROC Curve for Training Data\n",
    "    plt.figure(figsize=(7, 5))\n",
    "    for model_name, model in models.items():\n",
    "        if model_name == 'DNN':\n",
    "            train_pred_prob = model.predict(X_train).ravel()\n",
    "        else:\n",
    "            train_pred_prob = model.predict_proba(X_train)[:, 1]\n",
    "        \n",
    "        fpr_train, tpr_train, _ = roc_curve(Y_train, train_pred_prob)\n",
    "        plt.plot(fpr_train, tpr_train, label=f'{model_name}')\n",
    "    \n",
    "    plt.plot([0, 1], [0, 1], linestyle='--', color='black')  # Diagonal 45-degree line\n",
    "    plt.title('ROC Curve - Training Data')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.legend()\n",
    "    plt.savefig('roc_curve_train.png', dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "    # ROC Curve for Testing Data\n",
    "    plt.figure(figsize=(7, 5))\n",
    "    for model_name, model in models.items():\n",
    "        if model_name == 'DNN':\n",
    "            test_pred_prob = model.predict(X_test).ravel()\n",
    "        else:\n",
    "            test_pred_prob = model.predict_proba(X_test)[:, 1]\n",
    "        \n",
    "        fpr_test, tpr_test, _ = roc_curve(Y_test, test_pred_prob)\n",
    "        plt.plot(fpr_test, tpr_test, label=f'{model_name}')\n",
    "    \n",
    "    plt.plot([0, 1], [0, 1], linestyle='--', color='black')  # Diagonal 45-degree line\n",
    "    plt.title('ROC Curve - Testing Data')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.legend()\n",
    "    plt.savefig('roc_curve_test.png', dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "    # Precision-Recall Curve for Training Data\n",
    "    plt.figure(figsize=(7, 5))\n",
    "    for model_name, model in models.items():\n",
    "        if model_name == 'DNN':\n",
    "            train_pred_prob = model.predict(X_train).ravel()\n",
    "        else:\n",
    "            train_pred_prob = model.predict_proba(X_train)[:, 1]\n",
    "        \n",
    "        precision_train, recall_train, _ = precision_recall_curve(Y_train, train_pred_prob)\n",
    "        plt.plot(recall_train, precision_train, label=f'{model_name}')\n",
    "    # plt.plot([1, 0], [0, 1], linestyle='--', color='black')\n",
    "    plt.title('Precision-Recall Curve - Training Data')\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.legend()\n",
    "    plt.savefig('precision_recall_curve_train.png', dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "    # Precision-Recall Curve for Testing Data\n",
    "    plt.figure(figsize=(7, 5))\n",
    "    for model_name, model in models.items():\n",
    "        if model_name == 'DNN':\n",
    "            test_pred_prob = model.predict(X_test).ravel()\n",
    "        else:\n",
    "            test_pred_prob = model.predict_proba(X_test)[:, 1]\n",
    "        \n",
    "        precision_test, recall_test, _ = precision_recall_curve(Y_test, test_pred_prob)\n",
    "        plt.plot(recall_test, precision_test, label=f'{model_name}')\n",
    "    # plt.plot([1, 0], [0, 1], linestyle='--', color='black')\n",
    "    plt.title('Precision-Recall Curve - Testing Data')\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.legend()\n",
    "    plt.savefig('precision_recall_curve_test.png', dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "    print(\"All plots have been saved as separate PNG files with 300 dpi.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b3cc86-5e1d-45d4-b2aa-929772491232",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting ROC and Precision-Recall curves...\n"
     ]
    }
   ],
   "source": [
    "print(\"Plotting ROC and Precision-Recall curves...\")\n",
    "plot_curves(fitted_models, X_train, X_test, Y_train, Y_test)\n",
    "print(\"Plots saved successfully.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
